max_seq_len: 2048
seed: 1
precision: "amp_bf16"

model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct

models:
-
  model_name: ${model_name_or_path}
  model:
    name: hf_causal_lm
    pretrained_model_name_or_path: ${model_name_or_path}
    pretrained: true
    # init_device: 'mixed'
    gptq_config:
      model_id: ${model_name_or_path}
      wbits: 4
      gs: 64
      actorder: True
      suffix: seed${seed}-True
      # dataset: /nfs/scistore19/alistgrp/amoeini/group_10_merged.txt
      # chunked: true
      # chunk_size: 1024
      clip: False
      mse: False
    use_auth_token: true
  trim_batch: true
  tokenizer:
    name: ${model_name_or_path}
    kwargs:
      model_max_length: ${max_seq_len}

loggers:
  wandb: 
    project: "DASLab"
    name: ${model_name_or_path}
  
# fsdp_config:
#   sharding_strategy: FULL_SHARD
#   mixed_precision: FULL
#   forward_prefetch: True
#   limit_all_gathers: True

device_eval_batch_size: 1

icl_tasks: "eval/yamls/tasks_v0.3.yaml"
# eval_gauntlet: "eval/yamls/eval_gauntlet_v0.3.yaml"
